# Machine learning

## DPR

## Bayes Model

## Random processes

### Distributions

# Deep Neural Networks

## Layers

### Deep

### Convolution

### Recurrent

### LSTM

### Graph

# REINFORCE

## Bellmann

### Bellman Equation for Value Functions

The **Bellman equation** for value functions captures the relationship between the value of a state and the values of subsequent states in a reinforcement learning context. It provides a recursive decomposition of the value function, which is central to solving reinforcement learning problems.

Given a policy \(\pi\), the value function \(V^\pi(s)\) of a state \(s\) under policy \(\pi\) is defined as the expected return starting from state \(s\), and then following policy \(\pi\). The Bellman equation for the value function under policy \(\pi\) is:

\[V^\pi(s) = \sum*{a \in A} \pi(a|s) \sum*{s', r} p(s', r | s, a) \left[ r + \gamma V^\pi(s') \right]\]

Where:

- \(V^\pi(s)\) is the value of state \(s\) under policy \(\pi\),
- \(\pi(a|s)\) denotes the probability of taking action \(a\) in state \(s\) under policy \(\pi\),
- \(p(s', r | s, a)\) is the transition probability to state \(s'\) with reward \(r\) after taking action \(a\) in state \(s\),
- \(\gamma\) is the discount factor, which quantifies the difference in importance between future rewards and present rewards.

### Bellman Optimality Equation

The **Bellman Optimality Equation** pertains to the optimal policy, defining the best possible value function, known as the optimal value function. It describes the value of the best action to take in a given state.

The Bellman Optimality Equation for the optimal value function \(V^\*(s)\) is:

\[V^_(s) = \max*{a \in A} \sum*{s', r} p(s', r | s, a) \left[ r + \gamma V^_(s') \right]\]

For the action-value function \(Q^\*(s, a)\), which represents the value of taking action \(a\) in state \(s\) and then following the optimal policy, the equation is:

\[Q^_(s, a) = \sum*{s', r} p(s', r | s, a) \left[ r + \gamma \max*{a'} Q^_(s', a') \right]\]

Where:

- \(V^_(s)\) and \(Q^_(s, a)\) are the optimal value and action-value functions, respectively,
- The \(\max\) operator is used to select the action that maximizes the expected return.

### Summary

The Bellman equations offer a recursive solution to determining value functions in reinforcement learning. They form the theoretical foundation for dynamic programming techniques such as Value Iteration and Policy Iteration, which seek to find optimal policies by improving value function estimates iteratively.

## Methodologies

## Training Methods

# Value Estimation

# Policy Estimation

# Policies

## of policy

Off-policy methods can learn about one policy (the target policy) from the experience generated by another policy (the behavior policy). This decoupling allows these methods to learn from historical data collected by previous policies or even from data generated by entirely different strategies. Experience replay, where past experiences are stored and randomly sampled to update the policy, is a technique often used in off-policy learning to improve sample efficiency and break the correlation between consecutive samples. Deep Q-Networks (DQN) and Deep Deterministic Policy Gradient (DDPG) are examples of off-policy methods.

## on policy

On-Policy Methods: In on-policy reinforcement learning, the policy being learned is the same policy used to select actions during interaction with the environment. The key characteristic of on-policy methods is that they require fresh samples from the environment generated by the current version of the policy. Any update to the policy necessitates new samples since the old samples do not accurately represent the updated policy's behavior. PPO is an example of an on-policy method.

# Value Iteration

# Q-Learning

# A2C (Advantage Actor Critic)

## Basics

### Advantage

loss or gain

### Actor

policy

### Critic

value or q value estimation

## DDPG (Deep Deterministic Policy Gradient)

## ACER

## TRPO

## PPO

# A3C Asynchronous Actor Critic
